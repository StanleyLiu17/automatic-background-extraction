{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic segmentation of aerial images with deep networks\n",
    "NOTE: THIS NOTEBOOK IS AN ADAPTATION SPECIFICALLY FOR TRAINING SEGNET. THIS BELONGS TO Nicolas Audebert, Bertrand Le Saux and Sébastien Lefèvre (https://github.com/nshaud/DeepNetsForEO)\n",
    "\n",
    "This notebook presents a straightforward PyTorch implementation of a Fully Convolutional Network for semantic segmentation of aerial images. More specifically, we aim to automatically perform scene interpretation of images taken from a plane or a satellite by classifying every pixel into several land cover classes.\n",
    "\n",
    "As a demonstration, we are going to use the [SegNet architecture](http://mi.eng.cam.ac.uk/projects/segnet/) to segment aerial images over the cities of Vaihingen and Potsdam. The images are from the [ISPRS 2D Semantic Labeling dataset](http://www2.isprs.org/commissions/comm3/wg4/results.html). We will train a network to segment roads, buildings, vegetation and cars.\n",
    "\n",
    "This work is a PyTorch implementation of the baseline presented in [\"Beyond RGB: Very High Resolution Urban Remote Sensing With Multimodal Deep Networks \"](https://hal.archives-ouvertes.fr/hal-01636145), *Nicolas Audebert*, *Bertrand Le Saux* and *Sébastien Lefèvre*, ISPRS Journal, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "This notebook requires a few useful libraries, e.g. `torch`, `scikit-image`, `numpy` and `matplotlib`. You can install everything using `pip install -r requirements.txt`.\n",
    "\n",
    "This is expected to run on GPU, and therefore you should use `torch` in combination with CUDA/cuDNN. This can probably be made to run on CPU but be warned that:\n",
    "  * you have to remove all calls to `torch.Tensor.cuda()` throughout this notebook,\n",
    "  * this will be very slow.\n",
    "  \n",
    "A \"small\" GPU should be enough, e.g. this runs fine on a 4.7GB Tesla K20m. It uses quite a lot of RAM as the dataset is stored in-memory (about 5GB for Vaihingen). You can spare some memory by disabling the caching below. 4GB should be more than enough without caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports and stuff\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from skimage.transform import rescale, resize\n",
    "from glob import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "import itertools\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler\n",
    "import torch.nn.init\n",
    "from torch.autograd import Variable\n",
    "from itertools import product\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "There are several parameters than can be tuned to use this notebook with different datasets. The default parameters are suitable for the ISPRS dataset, but you can change them to work with your data.\n",
    "\n",
    "### Examples\n",
    "\n",
    "  * Binary classification: `N_CLASSES = 2`\n",
    "  * Multi-spectral data (e.g. IRRGB): `IN_CHANNELS = 4`\n",
    "  * New folder naming convention : `DATA_FOLDER = MAIN_FOLDER + 'sentinel2/sentinel2_img_{}.tif'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "WINDOW_SIZE = (256, 256) # Patch size\n",
    "STRIDE = 32 # Stride for testing\n",
    "IN_CHANNELS = 3 # Number of input channels (e.g. RGB)\n",
    "FOLDER = \"./ISPRS_dataset/\" # Replace with your \"/path/to/the/ISPRS/dataset/folder/\"\n",
    "BATCH_SIZE = 10 # Number of samples in a mini-batch\n",
    "\n",
    "LABELS = ['unlabeled','ship','storage_tank','baseball_diamond','tennis_court','basketball_court','Ground_Track_Field','Bridge','Large_Vehicle','Small_Vehicle','Helicopter','Swimming_pool','Roundabout','Soccer_ball_field','plane','Harbor']\n",
    "category_dict = {'1':'ship', '2':'storage_tank','3':'baseball_diamond','4':'tennis_court','5':'basketball_court','6':'Ground_Track_Field','7':'Bridge','8':'Large_Vehicle','9':'Small_Vehicle','10':'Helicopter','11':'Swimming_pool','12':'Roundabout','13':'Soccer_ball_field','14':'plane','15':'Harbor'}\n",
    "N_CLASSES = len(LABELS) # Number of classes\n",
    "WEIGHTS = torch.ones(N_CLASSES) # Weights for class balancing\n",
    "CACHE = True # Store the dataset in-memory\n",
    "\n",
    "DATASET = 'iSAID'\n",
    "\n",
    "if DATASET == 'iSAID':\n",
    "    DATA_FOLDER = '/home/stanley2000120/dataset/RGB Images/{}.png'\n",
    "    LABEL_FOLDER = '/home/stanley2000120/dataset/Semantic_masks/images/{}_instance_color_RGB.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the dataset\n",
    "\n",
    "First, let's check that we are able to access the dataset and see what's going on. We use ```scikit-image``` for image manipulation.\n",
    "\n",
    "As the ISPRS dataset is stored with a ground truth in the RGB format, we need to define the color palette that can map the label id to its RGB color. We define two helper functions to convert from numeric to colors and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = {0: (  0,  0,  0), # Unlabeled\n",
    "           1: (  0,  0, 63), # Ship\n",
    "           2: (  0, 63, 63), # Storage Tank\n",
    "           3: (  0, 63,  0), # Baseball Diamond\n",
    "           4: (  0, 63,127), # Tennis Court\n",
    "           5: (  0, 63,191), # Basketball Court\n",
    "           6: (  0, 63,255), # Ground Track Field\n",
    "           7: (  0,127, 63), # Bridge\n",
    "           8: (0, 127, 127), # Large Vehicle\n",
    "           9: (  0,  0,127), # Small Vehicle\n",
    "           10: (  0,  0,191), # Helicopter\n",
    "           11: (  0,  0,255), # Swimming Pool\n",
    "           12: (  0,191,127), # Roundabout\n",
    "           13: (  0,127,191), # Soccer Ball Field\n",
    "           14: (  0,127,255), # Plane\n",
    "           15: (  0,100,155) # Harbor\n",
    "} \n",
    "invert_palette = {v: k for k, v in palette.items()}\n",
    "\n",
    "def convert_to_color(arr_2d, palette=palette):\n",
    "    \"\"\" Numeric labels to RGB-color encoding \"\"\"\n",
    "    arr_3d = np.zeros((arr_2d.shape[0], arr_2d.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "    for c, i in palette.items():\n",
    "        m = arr_2d == c\n",
    "        arr_3d[m] = i\n",
    "\n",
    "    return arr_3d\n",
    "\n",
    "def convert_from_color(arr_3d, palette=invert_palette):\n",
    "    \"\"\" RGB-color encoding to grayscale labels \"\"\"\n",
    "    arr_2d = np.zeros((arr_3d.shape[0], arr_3d.shape[1]), dtype=np.uint8)\n",
    "\n",
    "    for c, i in palette.items():\n",
    "        m = np.all(arr_3d == np.array(c).reshape(1, 1, 3), axis=2)\n",
    "        arr_2d[m] = i\n",
    "\n",
    "    return arr_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a bunch of utils functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "def conv_rgb():\n",
    "    files = os.listdir('A:/Downloads/Datasets/iSAID/train/RGB Images/')\n",
    "    for file in files:\n",
    "        image = Image.open(os.path.join('A:/Downloads/Datasets/iSAID/train/RGB Images/',file)).convert('RGB')\n",
    "        image.save(os.path.join('A:/Downloads/Datasets/iSAID/train/RGB Images/',file))\n",
    "\n",
    "def get_random_pos(img, window_shape):\n",
    "    \"\"\" Extract of 2D random patch of shape window_shape in the image \"\"\"\n",
    "    w, h = window_shape\n",
    "    W, H = img.shape[-2:]\n",
    "    x1 = random.randint(0, W - w - 1)\n",
    "    x2 = x1 + w\n",
    "    y1 = random.randint(0, H - h - 1)\n",
    "    y2 = y1 + h\n",
    "    return x1, x2, y1, y2\n",
    "\n",
    "def CrossEntropy2d(input, target, weight=None, size_average=True):\n",
    "    \"\"\" 2D version of the cross entropy loss \"\"\"\n",
    "    dim = input.dim()\n",
    "    if dim == 2:\n",
    "        return F.cross_entropy(input, target, weight, size_average)\n",
    "    elif dim == 4:\n",
    "        output = input.view(input.size(0),input.size(1), -1)\n",
    "        output = torch.transpose(output,1,2).contiguous()\n",
    "        output = output.view(-1,output.size(2))\n",
    "        target = target.view(-1)\n",
    "        return F.cross_entropy(output, target,weight, size_average)\n",
    "    else:\n",
    "        raise ValueError('Expected 2 or 4 dimensions (got {})'.format(dim))\n",
    "\n",
    "def accuracy(input, target):\n",
    "    return 100 * float(np.count_nonzero(input == target)) / target.size\n",
    "\n",
    "def sliding_window(top, step=10, window_size=(20,20)):\n",
    "    \"\"\" Slide a window_shape window across the image with a stride of step \"\"\"\n",
    "    for x in range(0, top.shape[0], step):\n",
    "        if x + window_size[0] > top.shape[0]:\n",
    "            x = top.shape[0] - window_size[0]\n",
    "        for y in range(0, top.shape[1], step):\n",
    "            if y + window_size[1] > top.shape[1]:\n",
    "                y = top.shape[1] - window_size[1]\n",
    "            yield x, y, window_size[0], window_size[1]\n",
    "            \n",
    "def count_sliding_window(top, step=10, window_size=(20,20)):\n",
    "    \"\"\" Count the number of windows in an image \"\"\"\n",
    "    c = 0\n",
    "    for x in range(0, top.shape[0], step):\n",
    "        if x + window_size[0] > top.shape[0]:\n",
    "            x = top.shape[0] - window_size[0]\n",
    "        for y in range(0, top.shape[1], step):\n",
    "            if y + window_size[1] > top.shape[1]:\n",
    "                y = top.shape[1] - window_size[1]\n",
    "            c += 1\n",
    "    return c\n",
    "\n",
    "def grouper(n, iterable):\n",
    "    \"\"\" Browse an iterator by chunk of n elements \"\"\"\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        chunk = tuple(itertools.islice(it, n))\n",
    "        if not chunk:\n",
    "            return\n",
    "        yield chunk\n",
    "\n",
    "def metrics(predictions, gts, label_values=LABELS):\n",
    "    cm = confusion_matrix(\n",
    "            gts,\n",
    "            predictions)#,\n",
    "            #range(len(label_values)))\n",
    "    \n",
    "    print(\"Confusion matrix :\")\n",
    "    print(cm)\n",
    "    \n",
    "    print(\"---\")\n",
    "    \n",
    "    # Compute global accuracy\n",
    "    total = sum(sum(cm))\n",
    "    accuracy = sum([cm[x][x] for x in range(len(cm))])\n",
    "    accuracy *= 100 / float(total)\n",
    "    print(\"{} pixels processed\".format(total))\n",
    "    print(\"Total accuracy : {}%\".format(accuracy))\n",
    "    \n",
    "    print(\"---\")\n",
    "    \n",
    "    # Compute F1 score\n",
    "    F1Score = np.zeros(len(label_values))\n",
    "    for i in range(len(label_values)):\n",
    "        try:\n",
    "            F1Score[i] = 2. * cm[i,i] / (np.sum(cm[i,:]) + np.sum(cm[:,i]))\n",
    "        except:\n",
    "            # Ignore exception if there is no element in class i for test set\n",
    "            pass\n",
    "    print(\"F1Score :\")\n",
    "    for l_id, score in enumerate(F1Score):\n",
    "        print(\"{}: {}\".format(label_values[l_id], score))\n",
    "\n",
    "    print(\"---\")\n",
    "        \n",
    "    # Compute kappa coefficient\n",
    "    total = np.sum(cm)\n",
    "    pa = np.trace(cm) / float(total)\n",
    "    pe = np.sum(np.sum(cm, axis=0) * np.sum(cm, axis=1)) / float(total*total)\n",
    "    kappa = (pa - pe) / (1 - pe);\n",
    "    print(\"Kappa: \" + str(kappa))\n",
    "    return accuracy\n",
    "\n",
    "def fill_gaps(values):\n",
    "    searchval=[255,0,255]\n",
    "    searchval2=[255,0,0,255]\n",
    "    idx=(np.array(np.where((values[:-2]==searchval[0]) & (values[1:-1]==searchval[1]) & (values[2:]==searchval[2])))+1)\n",
    "    idx2=(np.array(np.where((values[:-3]==searchval2[0]) & (values[1:-2]==searchval2[1]) & (values[2:-1]==searchval2[2]) & (values[3:]==searchval2[3])))+1)\n",
    "    idx3=(idx2+1)\n",
    "    new=idx.tolist()+idx2.tolist()+idx3.tolist()\n",
    "    newlist = [item for items in new for item in items]\n",
    "    values[newlist]=255\n",
    "    return values\n",
    "\n",
    "def fill_gaps2(values):\n",
    "    searchval=[0,255]\n",
    "    searchval2=[255,0]\n",
    "    idx=(np.array(np.where((values[:-1]==searchval[0]) & (values[1:]==searchval[1]))))\n",
    "    idx2=(np.array(np.where((values[:-1]==searchval[0]) & (values[1:]==searchval[1])))+1)\n",
    "    \n",
    "    new=idx.tolist()+idx2.tolist()\n",
    "    newlist = [item for items in new for item in items]\n",
    "    values[newlist]=255\n",
    "    return values\n",
    "\n",
    "def decode_segmap(image):\n",
    "                \n",
    "    r = np.zeros_like(image).astype(np.uint8)\n",
    "    for c, i in palette.items():\n",
    "        idx = image == 8\n",
    "        r[idx] = 255\n",
    "        idx = image == 9\n",
    "        r[idx] = 255\n",
    "    return np.array(r)\n",
    "\n",
    "def remove_objects(real_img,mask):\n",
    "    new_img = real_img.copy()\n",
    "    idx = mask == 255\n",
    "    new_img[idx] = 255\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "We define a PyTorch dataset (```torch.utils.data.Dataset```) that loads all the tiles in memory and performs random sampling. Tiles are stored in memory on the fly.\n",
    "\n",
    "The dataset also performs random data augmentation (horizontal and vertical flips) and normalizes the data in [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ISPRS_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ids, data_files=DATA_FOLDER, label_files=LABEL_FOLDER,\n",
    "                            cache=False, augmentation=True):\n",
    "        super(ISPRS_dataset, self).__init__()\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.cache = cache\n",
    "        \n",
    "        # List of files\n",
    "        self.data_files = [DATA_FOLDER.format(id) for id in ids]\n",
    "        self.label_files = [LABEL_FOLDER.format(id) for id in ids]\n",
    "\n",
    "        # Sanity check : raise an error if some files do not exist\n",
    "        for f in self.data_files + self.label_files:\n",
    "            if not os.path.isfile(f):\n",
    "                raise KeyError('{} is not a file !'.format(f))\n",
    "        \n",
    "        # Initialize cache dicts\n",
    "        self.data_cache_ = {}\n",
    "        self.label_cache_ = {}\n",
    "            \n",
    "    def __len__(self):\n",
    "        # Default epoch size is 10 000 samples\n",
    "        return 10000\n",
    "    \n",
    "    @classmethod\n",
    "    def data_augmentation(cls, *arrays, flip=True, mirror=True):\n",
    "        will_flip, will_mirror = False, False\n",
    "        if flip and random.random() < 0.5:\n",
    "            will_flip = True\n",
    "        if mirror and random.random() < 0.5:\n",
    "            will_mirror = True\n",
    "        \n",
    "        results = []\n",
    "        for array in arrays:\n",
    "            if will_flip:\n",
    "                if len(array.shape) == 2:\n",
    "                    array = array[::-1, :]\n",
    "                else:\n",
    "                    array = array[:, ::-1, :]\n",
    "            if will_mirror:\n",
    "                if len(array.shape) == 2:\n",
    "                    array = array[:, ::-1]\n",
    "                else:\n",
    "                    array = array[:, :, ::-1]\n",
    "            results.append(np.copy(array))\n",
    "            \n",
    "        return tuple(results)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # Pick a random image\n",
    "        random_idx = random.randint(0, len(self.data_files) - 1)\n",
    "        \n",
    "        # If the tile hasn't been loaded yet, put in cache\n",
    "        if random_idx in self.data_cache_.keys():\n",
    "            data = self.data_cache_[random_idx]\n",
    "        else:\n",
    "            # Data is normalized in [0, 1]\n",
    "            data = 1/255 * np.asarray(io.imread(self.data_files[random_idx]).transpose((2,0,1)), dtype='float32')\n",
    "        \n",
    "            if self.cache:\n",
    "                self.data_cache_[random_idx] = data\n",
    "            \n",
    "        if random_idx in self.label_cache_.keys():\n",
    "            label = self.label_cache_[random_idx]\n",
    "        else: \n",
    "            # Labels are converted from RGB to their numeric values\n",
    "            label = np.asarray(convert_from_color(io.imread(self.label_files[random_idx])), dtype='uint8')\n",
    "            if self.cache:\n",
    "                self.label_cache_[random_idx] = label\n",
    "\n",
    "        # Get a random patch\n",
    "        x1, x2, y1, y2 = get_random_pos(data, WINDOW_SIZE)\n",
    "        data_p = data[:, x1:x2,y1:y2]\n",
    "        label_p = label[x1:x2,y1:y2]\n",
    "        \n",
    "        # Data augmentation\n",
    "        data_p, label_p = self.data_augmentation(data_p, label_p)\n",
    "\n",
    "        # Return the torch.Tensor values\n",
    "        return (torch.from_numpy(data_p),\n",
    "                torch.from_numpy(label_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network definition\n",
    "\n",
    "We can now define the Fully Convolutional network based on the SegNet architecture. We could use any other network as drop-in replacement, provided that the output has dimensions `(N_CLASSES, W, H)` where `W` and `H` are the sliding window dimensions (i.e. the network should preserve the spatial dimensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SegNet(nn.Module):\n",
    "    # SegNet network\n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.kaiming_normal(m.weight.data)\n",
    "    \n",
    "    def __init__(self, in_channels=IN_CHANNELS, out_channels=N_CLASSES):\n",
    "        super(SegNet, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(2, return_indices=True)\n",
    "        self.unpool = nn.MaxUnpool2d(2)\n",
    "        \n",
    "        self.conv1_1 = nn.Conv2d(in_channels, 64, 3, padding=1)\n",
    "        self.conv1_1_bn = nn.BatchNorm2d(64)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.conv1_2_bn = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv2_1_bn = nn.BatchNorm2d(128)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.conv2_2_bn = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.conv3_1_bn = nn.BatchNorm2d(256)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.conv3_2_bn = nn.BatchNorm2d(256)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.conv3_3_bn = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv4_1 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.conv4_1_bn = nn.BatchNorm2d(512)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv4_2_bn = nn.BatchNorm2d(512)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv4_3_bn = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv5_1 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv5_1_bn = nn.BatchNorm2d(512)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv5_2_bn = nn.BatchNorm2d(512)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv5_3_bn = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv5_3_D = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv5_3_D_bn = nn.BatchNorm2d(512)\n",
    "        self.conv5_2_D = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv5_2_D_bn = nn.BatchNorm2d(512)\n",
    "        self.conv5_1_D = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv5_1_D_bn = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv4_3_D = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv4_3_D_bn = nn.BatchNorm2d(512)\n",
    "        self.conv4_2_D = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv4_2_D_bn = nn.BatchNorm2d(512)\n",
    "        self.conv4_1_D = nn.Conv2d(512, 256, 3, padding=1)\n",
    "        self.conv4_1_D_bn = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv3_3_D = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.conv3_3_D_bn = nn.BatchNorm2d(256)\n",
    "        self.conv3_2_D = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.conv3_2_D_bn = nn.BatchNorm2d(256)\n",
    "        self.conv3_1_D = nn.Conv2d(256, 128, 3, padding=1)\n",
    "        self.conv3_1_D_bn = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv2_2_D = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.conv2_2_D_bn = nn.BatchNorm2d(128)\n",
    "        self.conv2_1_D = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        self.conv2_1_D_bn = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv1_2_D = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.conv1_2_D_bn = nn.BatchNorm2d(64)\n",
    "        self.conv1_1_D = nn.Conv2d(64, out_channels, 3, padding=1)\n",
    "        \n",
    "        self.apply(self.weight_init)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder block 1\n",
    "        x = self.conv1_1_bn(F.relu(self.conv1_1(x)))\n",
    "        x = self.conv1_2_bn(F.relu(self.conv1_2(x)))\n",
    "        x, mask1 = self.pool(x)\n",
    "        \n",
    "        # Encoder block 2\n",
    "        x = self.conv2_1_bn(F.relu(self.conv2_1(x)))\n",
    "        x = self.conv2_2_bn(F.relu(self.conv2_2(x)))\n",
    "        x, mask2 = self.pool(x)\n",
    "        \n",
    "        # Encoder block 3\n",
    "        x = self.conv3_1_bn(F.relu(self.conv3_1(x)))\n",
    "        x = self.conv3_2_bn(F.relu(self.conv3_2(x)))\n",
    "        x = self.conv3_3_bn(F.relu(self.conv3_3(x)))\n",
    "        x, mask3 = self.pool(x)\n",
    "        \n",
    "        # Encoder block 4\n",
    "        x = self.conv4_1_bn(F.relu(self.conv4_1(x)))\n",
    "        x = self.conv4_2_bn(F.relu(self.conv4_2(x)))\n",
    "        x = self.conv4_3_bn(F.relu(self.conv4_3(x)))\n",
    "        x, mask4 = self.pool(x)\n",
    "        \n",
    "        # Encoder block 5\n",
    "        x = self.conv5_1_bn(F.relu(self.conv5_1(x)))\n",
    "        x = self.conv5_2_bn(F.relu(self.conv5_2(x)))\n",
    "        x = self.conv5_3_bn(F.relu(self.conv5_3(x)))\n",
    "        x, mask5 = self.pool(x)\n",
    "        \n",
    "        # Decoder block 5\n",
    "        x = self.unpool(x, mask5)\n",
    "        x = self.conv5_3_D_bn(F.relu(self.conv5_3_D(x)))\n",
    "        x = self.conv5_2_D_bn(F.relu(self.conv5_2_D(x)))\n",
    "        x = self.conv5_1_D_bn(F.relu(self.conv5_1_D(x)))\n",
    "        \n",
    "        # Decoder block 4\n",
    "        x = self.unpool(x, mask4)\n",
    "        x = self.conv4_3_D_bn(F.relu(self.conv4_3_D(x)))\n",
    "        x = self.conv4_2_D_bn(F.relu(self.conv4_2_D(x)))\n",
    "        x = self.conv4_1_D_bn(F.relu(self.conv4_1_D(x)))\n",
    "        \n",
    "        # Decoder block 3\n",
    "        x = self.unpool(x, mask3)\n",
    "        x = self.conv3_3_D_bn(F.relu(self.conv3_3_D(x)))\n",
    "        x = self.conv3_2_D_bn(F.relu(self.conv3_2_D(x)))\n",
    "        x = self.conv3_1_D_bn(F.relu(self.conv3_1_D(x)))\n",
    "        \n",
    "        # Decoder block 2\n",
    "        x = self.unpool(x, mask2)\n",
    "        x = self.conv2_2_D_bn(F.relu(self.conv2_2_D(x)))\n",
    "        x = self.conv2_1_D_bn(F.relu(self.conv2_1_D(x)))\n",
    "        \n",
    "        # Decoder block 1\n",
    "        x = self.unpool(x, mask1)\n",
    "        x = self.conv1_2_D_bn(F.relu(self.conv1_2_D(x)))\n",
    "        x = F.log_softmax(self.conv1_1_D(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now instantiate the network using the specified parameters. By default, the weights will be initialized using the [He policy](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instantiate the network\n",
    "net = SegNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download and load the pre-trained weights from VGG-16 on ImageNet. This step is optional but it makes the network converge faster. We skip the weights from VGG-16 that have no counterpart in SegNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    from urllib.request import URLopener\n",
    "except ImportError:\n",
    "    from urllib import URLopener\n",
    "\n",
    "# Download VGG-16 weights from PyTorch\n",
    "vgg_url = 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth'\n",
    "if not os.path.isfile('./vgg16_bn-6c64b313.pth'):\n",
    "    weights = URLopener().retrieve(vgg_url, './vgg16_bn-6c64b313.pth')\n",
    "\n",
    "vgg16_weights = torch.load('./vgg16_bn-6c64b313.pth')\n",
    "mapped_weights = {}\n",
    "for k_vgg, k_segnet in zip(vgg16_weights.keys(), net.state_dict().keys()):\n",
    "    if \"features\" in k_vgg:\n",
    "        mapped_weights[k_segnet] = vgg16_weights[k_vgg]\n",
    "        print(\"Mapping {} to {}\".format(k_vgg, k_segnet))\n",
    "        \n",
    "try:\n",
    "    net.load_state_dict(mapped_weights)\n",
    "    print(\"Loaded VGG-16 weights in SegNet !\")\n",
    "except:\n",
    "    # Ignore missing keys\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load the network on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "We now create a train/test split. If you want to use another dataset, you have to adjust the method to collect all filenames. In our case, we specify a fixed train/test split for the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if DATASET == 'iSAID':\n",
    "    l = os.listdir('/home/stanley2000120/dataset/RGB Images/')\n",
    "    train_ids=[x.split('.')[0] for x in l]\n",
    "    \n",
    "train_set = ISPRS_dataset(train_ids, cache=CACHE)\n",
    "train_loader = torch.utils.data.DataLoader(train_set,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing the optimizer\n",
    "\n",
    "We use the standard Stochastic Gradient Descent algorithm to optimize the network's weights.\n",
    "\n",
    "The encoder is trained at half the learning rate of the decoder, as we rely on the pre-trained VGG-16 weights. We use the ``torch.optim.lr_scheduler`` to reduce the learning rate by 10 after 25, 35 and 45 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "base_lr = 0.01\n",
    "params_dict = dict(net.named_parameters())\n",
    "params = []\n",
    "for key, value in params_dict.items():\n",
    "    if '_D' in key:\n",
    "        # Decoder weights are trained at the nominal learning rate\n",
    "        params += [{'params':[value],'lr': base_lr}]\n",
    "    else:\n",
    "        # Encoder weights are trained at lr / 2 (we have VGG-16 weights as initialization)\n",
    "        params += [{'params':[value],'lr': base_lr / 2}]\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=base_lr, momentum=0.9, weight_decay=0.0005)\n",
    "# We define the scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [25, 35, 45], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "def train(net, optimizer, epochs, scheduler=None, weights=WEIGHTS, save_epoch = 5):\n",
    "    losses = np.zeros(1000000)\n",
    "    mean_losses = np.zeros(100000000)\n",
    "    weights = weights.cuda()\n",
    "\n",
    "    iter_ = 0\n",
    "    \n",
    "    for e in range(1, epochs + 1):\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        net.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "            optimizer.zero_grad()\n",
    "            output = net(data)\n",
    "            loss = CrossEntropy2d(output, target, weight=weights)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses[iter_] = loss.item()\n",
    "            mean_losses[iter_] = np.mean(losses[max(0,iter_-100):iter_])\n",
    "            \n",
    "            if iter_ % 100 == 0:\n",
    "                clear_output()\n",
    "                #rgb = np.asarray(255 * np.transpose(data.data.cpu().numpy()[0],(1,2,0)), dtype='uint8')\n",
    "                pred = np.argmax(output.data.cpu().numpy()[0], axis=0)\n",
    "                gt = target.data.cpu().numpy()[0]\n",
    "                print('Train (epoch {}/{}) [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {}'.format(\n",
    "                    e, epochs, batch_idx, len(train_loader),\n",
    "                    100. * batch_idx / len(train_loader), loss.item(), accuracy(pred, gt)))\n",
    "                \n",
    "            iter_ += 1\n",
    "            \n",
    "            del(data, target, loss)\n",
    "            \n",
    "        if e % save_epoch == 0:\n",
    "            # We validate with the largest possible stride for faster computing\n",
    "            #acc = test(net, test_ids, all=False, stride=min(WINDOW_SIZE))\n",
    "            #torch.save(net.state_dict(), './segnet256_epoch{}_{}'.format(e, acc))\n",
    "            torch.save(net.state_dict(), './segnet256_epoch{}'.format(e))\n",
    "    torch.save(net.state_dict(), './segnet_final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network\n",
    "\n",
    "Let's train the network for 50 epochs. The `matplotlib` graph is periodically udpated with the loss plot and a sample inference. Depending on your GPU, this might take from a few hours (Titan Pascal) to a full day (old K20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(net, optimizer, 50, scheduler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
